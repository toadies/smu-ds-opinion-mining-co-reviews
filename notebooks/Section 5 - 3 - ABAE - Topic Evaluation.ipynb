{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the existing topics\n",
    "Review misspelled words\n",
    "Calculate Coherance Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "module_path = os.path.abspath(os.path.join(\"..\"))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path+\"/scripts\")\n",
    "    sys.path.append(module_path+\"/classes\")\n",
    "\n",
    "review_corpus_path = \"../data/tech_review_sent_corpus.pkl\"\n",
    "vocab_path = \"../data/glove-tech-revew-vocab.txt\"\n",
    "emb_filename = '../models/w2v_embedding'\n",
    "aspect_file_path = \"../results/aspect.json\"\n",
    "aspect_model_path = \"../results/model_param\"\n",
    "vocab_path = \"../data/vocab-text-review.txt\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['Aspect 0', 'Aspect 1', 'Aspect 2', 'Aspect 3', 'Aspect 4', 'Aspect 5', 'Aspect 6', 'Aspect 7', 'Aspect 8', 'Aspect 9', 'Aspect 10', 'Aspect 11', 'Aspect 12', 'Aspect 13', 'Aspect 14'])\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(\"../results/ABAE/test-abae-k-15-orth-0.8.json\") as f:\n",
    "    aspects = json.load(f)\n",
    "    \n",
    "print(aspects.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get topics words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aspect 0\n",
      "Aspect 1\n",
      "Aspect 2\n",
      "Aspect 3\n",
      "Aspect 4\n",
      "Aspect 5\n",
      "Aspect 6\n",
      "Aspect 7\n",
      "Aspect 8\n",
      "Aspect 9\n",
      "Aspect 10\n",
      "Aspect 11\n",
      "Aspect 12\n",
      "Aspect 13\n",
      "Aspect 14\n"
     ]
    }
   ],
   "source": [
    "aspect_jara = []\n",
    "for aspect, words in aspects.items():   \n",
    "    data = {}\n",
    "    print(aspect)\n",
    "    for word, score in words.items():\n",
    "        for alt_word, alt_score in words.items():\n",
    "            score = jaro_winkler_similarity(word, alt_word)\n",
    "            if (score >= .9) & (score < 1):\n",
    "                try:\n",
    "                    data[word][\"alt_words\"][alt_word] = score\n",
    "                except:\n",
    "                    data[word] = {\n",
    "                        \"aspect\":aspect,\n",
    "                        \"alt_words\": {}\n",
    "                    }\n",
    "                data[word][\"alt_words\"][alt_word] = score\n",
    "    aspect_jara.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(204, 3)\n"
     ]
    }
   ],
   "source": [
    "col_1 = []\n",
    "col_2 = []\n",
    "col_3 = []\n",
    "for i, data in enumerate(aspect_jara):\n",
    "    for key, alt in data.items():\n",
    "        for word in alt[\"alt_words\"].items():\n",
    "            col_1.append(key)\n",
    "            col_2.append(word[0])\n",
    "            col_3.append(alt[\"aspect\"])\n",
    "            \n",
    "\n",
    "df = pd.DataFrame([col_1, col_2, col_3]).T.rename(columns={0:\"col_1\",1:\"col_2\", 2:\"col_3\"})\n",
    "df = df.sort_values(by=[\"col_3\",\"col_1\"])\n",
    "print(df.shape)\n",
    "\n",
    "# for i, row in df.iterrows():\n",
    "#     print( '\"' + str(row.col_1) +'\": \"' +str(row.col_2) +'\",')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab = []\n",
    "# for aspect, words in aspects.items():\n",
    "#     for word, score in words.items():\n",
    "#         vocab.append(word)\n",
    "\n",
    "# vocab = list(set(vocab))\n",
    "# vocab.sort()\n",
    "\n",
    "# from nltk.metrics.distance import edit_distance, jaro_winkler_similarity\n",
    "\n",
    "# data = {}\n",
    "# for i, word in enumerate(vocab):\n",
    "#     for j, alt_word in enumerate(vocab):\n",
    "#         score = jaro_winkler_similarity(word, alt_word)\n",
    "#         if (score >= .9) & (score < 1):\n",
    "#             try:\n",
    "#                 data[word][alt_word] = score\n",
    "#             except:\n",
    "#                 data[word] = {}\n",
    "#                 data[word][alt_word] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create coherance score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "148446\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "review_corpus_path = \"../data/tech_review_sent_corpus.pkl\"\n",
    "\n",
    "with open(review_corpus_path,\"rb\") as f:\n",
    "    tech_review_corpus = pickle.load(f)\n",
    "\n",
    "print(len(tech_review_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'ABAE.reader' from '/Users/christopherballenger/Code/smu-ds-opinion-mining-co-reviews/classes/ABAE/reader.py'>"
      ]
     },
     "execution_count": 396,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(ABAE.reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 148446/148446 [00:06<00:00, 22169.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Creating vocab ...\n",
      "   963539 total words, 22093 unique words\n",
      " Reading dataset ...\n",
      "  train set\n",
      "Corpus Size 148446\n",
      "Total Document Analyzed 148446\n",
      "<num> hit rate: 1.22%, <unk> hit rate: 0.00%\n",
      "148446\n"
     ]
    }
   ],
   "source": [
    "import ABAE.reader as dataset\n",
    "\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool\n",
    "\n",
    "with open(\"../data/stop_words.json\", \"r\") as f:\n",
    "  stop_words = json.load(f)\n",
    "\n",
    "def removeStopWords(review):\n",
    "  tokens = review.split(\" \")\n",
    "  return \" \".join([ word for word in tokens if word not in stop_words ])\n",
    "\n",
    "reviews = pd.DataFrame(tech_review_corpus).review.tolist()\n",
    "\n",
    "with Pool() as p:\n",
    "  reviews = list(tqdm(p.imap(removeStopWords, reviews), total=len(reviews)))\n",
    "\n",
    "vocab, train_x, overall_maxlen = dataset.get_data(reviews, vocab_path, vocab_size=0, maxlen=115)\n",
    "\n",
    "vocab_inv = {}\n",
    "for w, ind in vocab.items():\n",
    "    vocab_inv[ind] = w\n",
    "    \n",
    "processed_docs = []\n",
    "for review in train_x:\n",
    "    processed_docs.append([vocab_inv[i] for i in review ])\n",
    "    \n",
    "print(len(train_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "\n",
    "dictionary = Dictionary(processed_docs)\n",
    "dictionary.add_documents([[\"<num>\",\"<unk>\"]])\n",
    "\n",
    "corpus = [dictionary.doc2bow(text) for text in processed_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['management',\n",
       "  'management',\n",
       "  'wa',\n",
       "  'fair',\n",
       "  'impartial',\n",
       "  'regard',\n",
       "  'request',\n",
       "  'assistance',\n",
       "  'emergency',\n",
       "  'time',\n",
       "  'personally',\n",
       "  'learn',\n",
       "  'latter',\n",
       "  'true',\n",
       "  'co',\n",
       "  'work',\n",
       "  'diverse',\n",
       "  'environment',\n",
       "  'everyone',\n",
       "  'brought',\n",
       "  'unique',\n",
       "  'trait',\n",
       "  'table',\n",
       "  'knowledge',\n",
       "  'wa',\n",
       "  'easilly',\n",
       "  'shared',\n",
       "  'one',\n",
       "  'technician',\n",
       "  'another',\n",
       "  'hard',\n",
       "  'part',\n",
       "  'job',\n",
       "  'trying',\n",
       "  'keep',\n",
       "  'flow',\n",
       "  'moving',\n",
       "  'resolve',\n",
       "  'issue',\n",
       "  'forward',\n",
       "  'support',\n",
       "  'without',\n",
       "  'going',\n",
       "  'past',\n",
       "  'estimated',\n",
       "  'time',\n",
       "  'limit',\n",
       "  '<num>',\n",
       "  'minute',\n",
       "  'enjoy',\n",
       "  'part',\n",
       "  'love',\n",
       "  'could',\n",
       "  'fix',\n",
       "  'issue',\n",
       "  'customer',\n",
       "  'therein',\n",
       "  'true',\n",
       "  'satisfactory',\n",
       "  'technician',\n",
       "  'know',\n",
       "  'wa',\n",
       "  'issue',\n",
       "  'part',\n",
       "  'make',\n",
       "  'customer',\n",
       "  'easier',\n",
       "  'get',\n",
       "  'duty',\n",
       "  'completed']]"
      ]
     },
     "execution_count": 408,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[ review for review in processed_docs if \"therein\" in review ]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from gensim.models import CoherenceModel\n",
    "import numpy as np\n",
    "\n",
    "def calc_coherance(topics):\n",
    "    u_mass = []\n",
    "    flags = []\n",
    "    for n, topic in enumerate(topics):\n",
    "        cm = CoherenceModel(topics=[topic], corpus=corpus, dictionary=dictionary, coherence='u_mass')\n",
    "        u_mass.append(cm.get_coherence())\n",
    "        return np.mean(u_mass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "files = [x for x in os.walk(\"../results/ABAE\")]\n",
    "\n",
    "scores = []\n",
    "for t in files[0][2]:\n",
    "    if (t[:6] == \"abae-k\") & (t[-4:] == 'json') :\n",
    "        with open(os.path.join(\"../results/ABAE\",t)) as f:\n",
    "            aspects = json.load(f)\n",
    "\n",
    "        topics = [[ word for word, score in words.items() ] for aspect, words in aspects.items() ]\n",
    "\n",
    "        s = calc_coherance(topics)\n",
    "        scores.append((t, s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "for score in scores:\n",
    "    x = score[0][:-5].split(\"-\")\n",
    "    result.append({\n",
    "        \"k\":x[2],\n",
    "        \"ortho\":x[4],\n",
    "        \"score\":score[1]\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(result).to_csv(\"../data/abae_umass.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>k</th>\n",
       "      <th>ortho</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>-15.745435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15</td>\n",
       "      <td>0.8</td>\n",
       "      <td>-15.876360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-16.937292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11</td>\n",
       "      <td>0.8</td>\n",
       "      <td>-16.039723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-17.109498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>11</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-12.992026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>-8.335060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>9</td>\n",
       "      <td>0.8</td>\n",
       "      <td>-16.989177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>15</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-16.754466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>7</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-13.089316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>-18.030910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>0.8</td>\n",
       "      <td>-16.068166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-16.117505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>-15.864582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>7</td>\n",
       "      <td>0.8</td>\n",
       "      <td>-15.921789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>9</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-16.735608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>-14.771769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>-16.830434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>11</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-11.506859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>15</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-9.187829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>7</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-14.337708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>-17.185711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>5</td>\n",
       "      <td>0.8</td>\n",
       "      <td>-12.395389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-16.960662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>14</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-16.976217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>10</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-10.629213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-12.972654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>13</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-17.432678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>8</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-15.052218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>6</td>\n",
       "      <td>0.8</td>\n",
       "      <td>-11.714436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>-13.317117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>6</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-12.628898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>14</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-17.090947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>-16.817017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>-14.472158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>10</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-10.308224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>13</td>\n",
       "      <td>0.8</td>\n",
       "      <td>-17.300870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>10</td>\n",
       "      <td>0.8</td>\n",
       "      <td>-13.167273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>8</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-15.313140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>-16.435922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>14</td>\n",
       "      <td>0.8</td>\n",
       "      <td>-15.832647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-13.514956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>-12.902337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>13</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-16.815364</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     k ortho      score\n",
       "0    9     1 -15.745435\n",
       "1   15   0.8 -15.876360\n",
       "2    9   0.5 -16.937292\n",
       "3   11   0.8 -16.039723\n",
       "4   12   0.5 -17.109498\n",
       "5   11   0.5 -12.992026\n",
       "6    5     1  -8.335060\n",
       "7    9   0.8 -16.989177\n",
       "8   15   0.5 -16.754466\n",
       "9    7   0.3 -13.089316\n",
       "10  12     1 -18.030910\n",
       "11  12   0.8 -16.068166\n",
       "12  12   0.3 -16.117505\n",
       "13  11     1 -15.864582\n",
       "14   7   0.8 -15.921789\n",
       "15   9   0.3 -16.735608\n",
       "16   6     1 -14.771769\n",
       "17  14     1 -16.830434\n",
       "18  11   0.3 -11.506859\n",
       "19  15   0.3  -9.187829\n",
       "20   7   0.5 -14.337708\n",
       "21  13     1 -17.185711\n",
       "22   5   0.8 -12.395389\n",
       "23   6   0.5 -16.960662\n",
       "24  14   0.3 -16.976217\n",
       "25  10   0.3 -10.629213\n",
       "26   5   0.5 -12.972654\n",
       "27  13   0.3 -17.432678\n",
       "28   8   0.3 -15.052218\n",
       "29   6   0.8 -11.714436\n",
       "30   8     1 -13.317117\n",
       "31   6   0.3 -12.628898\n",
       "32  14   0.5 -17.090947\n",
       "33  15     1 -16.817017\n",
       "34   8   0.8 -14.472158\n",
       "35  10   0.5 -10.308224\n",
       "36  13   0.8 -17.300870\n",
       "37  10   0.8 -13.167273\n",
       "38   8   0.5 -15.313140\n",
       "39   7     1 -16.435922\n",
       "40  14   0.8 -15.832647\n",
       "41   5   0.3 -13.514956\n",
       "42  10     1 -12.902337\n",
       "43  13   0.5 -16.815364"
      ]
     },
     "execution_count": 413,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "LDA Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "k = 5\n",
    "ortho = 0.8\n",
    "with open(\"../results/ABAE/abae-k-{0}-orth-{1}.json\".format(str(k), str(ortho)), \"r\") as f:\n",
    "    apsects = json.load(f)\n",
    "    \n",
    "topics = [[ word for word, score in words.items() ] for aspect, words in apsects.items() ]\n",
    "\n",
    "topics_idx = [[ vocab[word] for word, score in words.items() ] for aspect, words in apsects.items() ] \n",
    "\n",
    "len(topics_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "vocab = Counter()\n",
    "\n",
    "for topic in topics:\n",
    "    for word in topic:\n",
    "        vocab[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('awesome', 2),\n",
       " ('great', 2),\n",
       " ('wonderful', 2),\n",
       " ('good', 2),\n",
       " ('management', 2),\n",
       " ('funny', 1),\n",
       " ('friendly', 1),\n",
       " ('really', 1),\n",
       " ('kind', 1),\n",
       " ('coworker', 1)]"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def termFreq(doc):\n",
    "    vocab = Counter()\n",
    "    for word in doc:\n",
    "        vocab[word] += 1\n",
    "        \n",
    "    return dict(vocab)\n",
    "\n",
    "reviews_tf = list(map(termFreq, train_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:11<00:00,  2.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta Shape (148425, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "document_sums = np.zeros((len(topics_idx),k))\n",
    "\n",
    "def countWords(topic, doc):\n",
    "    x = 0\n",
    "    for w in topic:\n",
    "        try:\n",
    "            x += doc[w]\n",
    "        except:\n",
    "            pass\n",
    "    return x\n",
    "\n",
    "for row, topic in tqdm(enumerate(topics_idx), total=len(topics_idx)):\n",
    "    for col, doc in enumerate(reviews_tf):\n",
    "        ct = countWords(topic, doc)\n",
    "        document_sums[row,col] = ct\n",
    "\n",
    "alpha = 1/k\n",
    "theta = []\n",
    "for i in range(k):\n",
    "    if document_sums[i,:].sum() == 0: break\n",
    "    theta.append(( document_sums[i,:] + alpha ) / document_sums[i,:].sum())\n",
    "    \n",
    "theta = np.array(theta).T\n",
    "\n",
    "print(\"Theta Shape\", theta.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 22791)"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_sums = np.zeros((len(vocab), k))\n",
    "\n",
    "def countWords(topic, doc):\n",
    "    x = 0\n",
    "    for w in topic:\n",
    "        try:\n",
    "            x += doc[w]\n",
    "        except:\n",
    "            pass\n",
    "    return x\n",
    "\n",
    "for row, topic in tqdm(enumerate(topics_idx), total=len(topics_idx)):\n",
    "    for col, doc in enumerate(reviews_tf):\n",
    "        ct = countWords(topic, doc)\n",
    "        document_sums[row,col] = ct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/derekgreene/topic-model-tutorial/blob/master/3%20-%20Parameter%20Selection%20for%20NMF.ipynb\n",
    "import gensim\n",
    "from itertools import combinations\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "w2v_model = gensim.models.Word2Vec.load(emb_filename)\n",
    "\n",
    "def getVector(w2v_model, word):\n",
    "    try:\n",
    "        v = w2v_model.wv[word]\n",
    "    except:\n",
    "        v = np.repeat(.00000001, w2v_model.vector_size)\n",
    "    return v\n",
    "\n",
    "def calculate_coherence( w2v_model, term_rankings ):\n",
    "    overall_coherence = 0.0\n",
    "    for topic_index in range(len(term_rankings)):\n",
    "        # check each pair of terms\n",
    "        pair_scores = []\n",
    "        for pair in combinations( term_rankings[topic_index], 2 ):\n",
    "            pair_scores.append( 1- cosine(getVector(w2v_model, pair[0]), getVector(w2v_model,pair[1])) )\n",
    "        # get the mean for all pairs in this topic\n",
    "        topic_score = sum(pair_scores) / len(pair_scores)\n",
    "        overall_coherence += topic_score\n",
    "    # get the mean score across all topics\n",
    "    return overall_coherence / len(term_rankings)\n",
    "\n",
    "calculate_coherence(w2v_model, topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk import WordNetLemmatizer\n",
    "\n",
    "lmtzr = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'night'"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lmtzr.lemmatize(\"nights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
