{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the existing topics\n",
    "Review misspelled words\n",
    "Calculate Coherance Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "module_path = os.path.abspath(os.path.join(\"..\"))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path+\"/scripts\")\n",
    "    sys.path.append(module_path+\"/classes\")\n",
    "\n",
    "review_corpus_path = \"../data/tech_review_sent_corpus.pkl\"\n",
    "vocab_path = \"../data/glove-tech-revew-vocab.txt\"\n",
    "emb_filename = '../models/w2v_embedding'\n",
    "aspect_file_path = \"../results/aspect.json\"\n",
    "aspect_model_path = \"../results/model_param\"\n",
    "vocab_path = \"../data/vocab-text-review.txt\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['Aspect 0', 'Aspect 1', 'Aspect 2', 'Aspect 3', 'Aspect 4', 'Aspect 5', 'Aspect 6', 'Aspect 7', 'Aspect 8', 'Aspect 9', 'Aspect 10', 'Aspect 11', 'Aspect 12', 'Aspect 13', 'Aspect 14'])\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(\"../results/ABAE/test-abae-k-15-orth-0.8.json\") as f:\n",
    "    aspects = json.load(f)\n",
    "    \n",
    "print(aspects.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get topics words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aspect 0\n",
      "Aspect 1\n",
      "Aspect 2\n",
      "Aspect 3\n",
      "Aspect 4\n",
      "Aspect 5\n",
      "Aspect 6\n",
      "Aspect 7\n",
      "Aspect 8\n",
      "Aspect 9\n",
      "Aspect 10\n",
      "Aspect 11\n",
      "Aspect 12\n",
      "Aspect 13\n",
      "Aspect 14\n"
     ]
    }
   ],
   "source": [
    "aspect_jara = []\n",
    "for aspect, words in aspects.items():   \n",
    "    data = {}\n",
    "    print(aspect)\n",
    "    for word, score in words.items():\n",
    "        for alt_word, alt_score in words.items():\n",
    "            score = jaro_winkler_similarity(word, alt_word)\n",
    "            if (score >= .9) & (score < 1):\n",
    "                try:\n",
    "                    data[word][\"alt_words\"][alt_word] = score\n",
    "                except:\n",
    "                    data[word] = {\n",
    "                        \"aspect\":aspect,\n",
    "                        \"alt_words\": {}\n",
    "                    }\n",
    "                data[word][\"alt_words\"][alt_word] = score\n",
    "    aspect_jara.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(204, 3)\n",
      "\"autonomously\": \"autonomy\",\n",
      "\"autonomy\": \"autonomously\",\n",
      "\"employeee\": \"employer\",\n",
      "\"employer\": \"employeee\",\n",
      "\"experice\": \"experience\",\n",
      "\"experience\": \"experice\",\n",
      "\"flexibiltiy\": \"flexibilty\",\n",
      "\"flexibiltiy\": \"flexiblity\",\n",
      "\"flexibiltiy\": \"flexible\",\n",
      "\"flexibilty\": \"flexibiltiy\",\n",
      "\"flexibilty\": \"flexiblity\",\n",
      "\"flexibilty\": \"flexible\",\n",
      "\"flexible\": \"flexibilty\",\n",
      "\"flexible\": \"flexibiltiy\",\n",
      "\"flexible\": \"flexiblity\",\n",
      "\"flexiblity\": \"flexibilty\",\n",
      "\"flexiblity\": \"flexibiltiy\",\n",
      "\"flexiblity\": \"flexible\",\n",
      "\"guidance\": \"guideance\",\n",
      "\"guideance\": \"guidance\",\n",
      "\"job\": \"jobe\",\n",
      "\"jobe\": \"job\",\n",
      "\"manager\": \"managerial\",\n",
      "\"managerial\": \"manager\",\n",
      "\"opportinity\": \"opportunity\",\n",
      "\"opportinity\": \"oppportunity\",\n",
      "\"opportunity\": \"opportinity\",\n",
      "\"opportunity\": \"oppportunity\",\n",
      "\"oppportunity\": \"opportinity\",\n",
      "\"oppportunity\": \"opportunity\",\n",
      "\"responsibile\": \"responsiblities\",\n",
      "\"responsiblities\": \"responsibile\",\n",
      "\"tipical\": \"tipically\",\n",
      "\"tipically\": \"tipical\",\n",
      "\"bought\": \"brought\",\n",
      "\"brought\": \"bought\",\n",
      "\"could\": \"couldnt\",\n",
      "\"couldnt\": \"could\",\n",
      "\"interviewed\": \"interviewer\",\n",
      "\"interviewer\": \"interviewed\",\n",
      "\"custom\": \"customer\",\n",
      "\"customer\": \"custom\",\n",
      "\"document\": \"documentation\",\n",
      "\"documentation\": \"document\",\n",
      "\"ensuring\": \"insuring\",\n",
      "\"financial\": \"finical\",\n",
      "\"finical\": \"financial\",\n",
      "\"insuring\": \"ensuring\",\n",
      "\"receipt\": \"recipient\",\n",
      "\"recipient\": \"receipt\",\n",
      "\"refund\": \"refunding\",\n",
      "\"refunding\": \"refund\",\n",
      "\"upmost\": \"utmost\",\n",
      "\"utmost\": \"upmost\",\n",
      "\"lane\": \"plane\",\n",
      "\"mind\": \"mindset\",\n",
      "\"mindset\": \"mind\",\n",
      "\"plane\": \"lane\",\n",
      "\"task\": \"tasking\",\n",
      "\"task\": \"tasker\",\n",
      "\"tasker\": \"task\",\n",
      "\"tasking\": \"task\",\n",
      "\"competition\": \"competitor\",\n",
      "\"competitor\": \"competition\",\n",
      "\"emloyee\": \"employee\",\n",
      "\"employee\": \"employer\",\n",
      "\"employee\": \"emloyee\",\n",
      "\"employer\": \"employee\",\n",
      "\"inclusion\": \"inclusive\",\n",
      "\"inclusive\": \"inclusion\",\n",
      "\"balace\": \"balance\",\n",
      "\"balace\": \"balanced\",\n",
      "\"balace\": \"balancegood\",\n",
      "\"balance\": \"balanced\",\n",
      "\"balance\": \"balace\",\n",
      "\"balance\": \"balancegood\",\n",
      "\"balanced\": \"balance\",\n",
      "\"balanced\": \"balace\",\n",
      "\"balanced\": \"balancegood\",\n",
      "\"balanced\": \"balenced\",\n",
      "\"balancegood\": \"balance\",\n",
      "\"balancegood\": \"balanced\",\n",
      "\"balancegood\": \"balace\",\n",
      "\"balenced\": \"balanced\",\n",
      "\"cultre\": \"culture\",\n",
      "\"cultre\": \"cultuer\",\n",
      "\"cultre\": \"culturegood\",\n",
      "\"cultre\": \"cuture\",\n",
      "\"cultre\": \"cultur\",\n",
      "\"cultuer\": \"culture\",\n",
      "\"cultuer\": \"cultre\",\n",
      "\"cultuer\": \"cuture\",\n",
      "\"cultuer\": \"cultur\",\n",
      "\"cultur\": \"culture\",\n",
      "\"cultur\": \"cultuer\",\n",
      "\"cultur\": \"culturegood\",\n",
      "\"cultur\": \"cultre\",\n",
      "\"cultur\": \"cuture\",\n",
      "\"culture\": \"cultuer\",\n",
      "\"culture\": \"culturegood\",\n",
      "\"culture\": \"cultre\",\n",
      "\"culture\": \"cuture\",\n",
      "\"culture\": \"cultur\",\n",
      "\"culturegood\": \"culture\",\n",
      "\"culturegood\": \"cultre\",\n",
      "\"culturegood\": \"cultur\",\n",
      "\"cuture\": \"culture\",\n",
      "\"cuture\": \"cultuer\",\n",
      "\"cuture\": \"cultre\",\n",
      "\"cuture\": \"cultur\",\n",
      "\"enivornment\": \"enivronment\",\n",
      "\"enivornment\": \"environment\",\n",
      "\"enivronment\": \"environment\",\n",
      "\"enivronment\": \"enivornment\",\n",
      "\"enivronment\": \"environmentally\",\n",
      "\"environment\": \"enivronment\",\n",
      "\"environment\": \"enivornment\",\n",
      "\"environment\": \"environmentally\",\n",
      "\"environmentally\": \"enivronment\",\n",
      "\"environmentally\": \"environment\",\n",
      "\"infracture\": \"infrastructre\",\n",
      "\"infracture\": \"infrastructure\",\n",
      "\"infrastructre\": \"infrastructure\",\n",
      "\"infrastructre\": \"infracture\",\n",
      "\"infrastructure\": \"infrastructre\",\n",
      "\"infrastructure\": \"infracture\",\n",
      "\"management\": \"management4\",\n",
      "\"management\": \"managemwnt\",\n",
      "\"management4\": \"management\",\n",
      "\"management4\": \"managemwnt\",\n",
      "\"managemwnt\": \"management\",\n",
      "\"managemwnt\": \"management4\",\n",
      "\"aloud\": \"loud\",\n",
      "\"ate\": \"mate\",\n",
      "\"ate\": \"late\",\n",
      "\"co\": \"cop\",\n",
      "\"colegues\": \"colleague\",\n",
      "\"colleague\": \"colegues\",\n",
      "\"cop\": \"co\",\n",
      "\"late\": \"ate\",\n",
      "\"loud\": \"loudly\",\n",
      "\"loud\": \"aloud\",\n",
      "\"loudly\": \"loud\",\n",
      "\"mate\": \"ate\",\n",
      "\"smoke\": \"smoked\",\n",
      "\"smoked\": \"smoke\",\n",
      "\"compile\": \"compiling\",\n",
      "\"compile\": \"completes\",\n",
      "\"compiling\": \"compile\",\n",
      "\"completes\": \"compile\",\n",
      "\"consult\": \"consulted\",\n",
      "\"consult\": \"consultation\",\n",
      "\"consultation\": \"consult\",\n",
      "\"consulted\": \"consult\",\n",
      "\"directed\": \"directs\",\n",
      "\"directs\": \"directed\",\n",
      "\"initiate\": \"initiating\",\n",
      "\"initiating\": \"initiate\",\n",
      "\"proposal\": \"propose\",\n",
      "\"propose\": \"proposal\",\n",
      "\"arrogance\": \"arrogant\",\n",
      "\"arrogant\": \"arrogance\",\n",
      "\"corrupt\": \"corruption\",\n",
      "\"corruption\": \"corrupt\",\n",
      "\"deceit\": \"deceitful\",\n",
      "\"deceitful\": \"deceit\",\n",
      "\"ignorance\": \"ignorant\",\n",
      "\"ignorant\": \"ignorance\",\n",
      "\"knowledgable\": \"knowledgeable\",\n",
      "\"knowledgeable\": \"knowledgable\",\n",
      "\"likable\": \"likeable\",\n",
      "\"likeable\": \"likable\",\n",
      "\"cover\": \"coverage\",\n",
      "\"coverage\": \"cover\",\n",
      "\"week\": \"weekend\",\n",
      "\"weekend\": \"week\",\n",
      "\"accustom\": \"accustomed\",\n",
      "\"accustomed\": \"accustom\",\n",
      "\"aspire\": \"aspires\",\n",
      "\"aspires\": \"aspire\",\n",
      "\"attracted\": \"attractive\",\n",
      "\"attracted\": \"attracts\",\n",
      "\"attractive\": \"attracted\",\n",
      "\"attractive\": \"attracts\",\n",
      "\"attracts\": \"attracted\",\n",
      "\"attracts\": \"attractive\",\n",
      "\"belief\": \"belives\",\n",
      "\"belives\": \"belief\",\n",
      "\"embrace\": \"embraced\",\n",
      "\"embraced\": \"embrace\",\n",
      "\"empowered\": \"empowers\",\n",
      "\"empowers\": \"empowered\",\n",
      "\"enable\": \"enables\",\n",
      "\"enables\": \"enable\",\n",
      "\"icon\": \"iconic\",\n",
      "\"iconic\": \"icon\",\n",
      "\"earn\": \"learn\",\n",
      "\"improve\": \"prove\",\n",
      "\"learn\": \"earn\",\n",
      "\"prove\": \"provide\",\n",
      "\"prove\": \"improve\",\n",
      "\"provide\": \"prove\",\n",
      "\"reach\": \"react\",\n",
      "\"react\": \"reach\",\n"
     ]
    }
   ],
   "source": [
    "col_1 = []\n",
    "col_2 = []\n",
    "col_3 = []\n",
    "for i, data in enumerate(aspect_jara):\n",
    "    for key, alt in data.items():\n",
    "        for word in alt[\"alt_words\"].items():\n",
    "            col_1.append(key)\n",
    "            col_2.append(word[0])\n",
    "            col_3.append(alt[\"aspect\"])\n",
    "            \n",
    "\n",
    "df = pd.DataFrame([col_1, col_2, col_3]).T.rename(columns={0:\"col_1\",1:\"col_2\", 2:\"col_3\"})\n",
    "df = df.sort_values(by=[\"col_3\",\"col_1\"])\n",
    "print(df.shape)\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    print( '\"' + str(row.col_1) +'\": \"' +str(row.col_2) +'\",')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab = []\n",
    "# for aspect, words in aspects.items():\n",
    "#     for word, score in words.items():\n",
    "#         vocab.append(word)\n",
    "\n",
    "# vocab = list(set(vocab))\n",
    "# vocab.sort()\n",
    "\n",
    "# from nltk.metrics.distance import edit_distance, jaro_winkler_similarity\n",
    "\n",
    "# data = {}\n",
    "# for i, word in enumerate(vocab):\n",
    "#     for j, alt_word in enumerate(vocab):\n",
    "#         score = jaro_winkler_similarity(word, alt_word)\n",
    "#         if (score >= .9) & (score < 1):\n",
    "#             try:\n",
    "#                 data[word][alt_word] = score\n",
    "#             except:\n",
    "#                 data[word] = {}\n",
    "#                 data[word][alt_word] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create coherance score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156991\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "review_corpus_path = \"../data/tech_review_sent_corpus.pkl\"\n",
    "\n",
    "with open(review_corpus_path,\"rb\") as f:\n",
    "    tech_review_corpus = pickle.load(f)\n",
    "\n",
    "print(len(tech_review_corpus))\n",
    "reviews = pd.DataFrame(tech_review_corpus).review.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'ABAE.reader' from '/Users/christopherballenger/Code/smu-ds-opinion-mining-co-reviews/classes/ABAE/reader.py'>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(ABAE.reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Creating vocab ...\n",
      "   944204 total words, 22788 unique words\n",
      " Reading dataset ...\n",
      "  train set\n",
      "Corpus Size 156991\n",
      "Total Document Analyzed 148425\n",
      "<num> hit rate: 1.24%, <unk> hit rate: 0.00%\n",
      "148425\n"
     ]
    }
   ],
   "source": [
    "import ABAE.reader as dataset\n",
    "\n",
    "vocab, train_x, overall_maxlen = dataset.get_data(reviews, vocab_path, vocab_size=0, maxlen=115)\n",
    "\n",
    "vocab_inv = {}\n",
    "for w, ind in vocab.items():\n",
    "    vocab_inv[ind] = w\n",
    "    \n",
    "processed_docs = []\n",
    "for review in train_x:\n",
    "    processed_docs.append([vocab_inv[i] for i in review ])\n",
    "    \n",
    "print(len(train_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 0:00:01.820760\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "\n",
    "dictionary = Dictionary(processed_docs)\n",
    "dictionary.add_documents([[\"<num>\",\"<unk>\"]])\n",
    "\n",
    "corpus = [dictionary.doc2bow(text) for text in processed_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from gensim.models import CoherenceModel\n",
    "import numpy as np\n",
    "\n",
    "def calc_coherance(topics):\n",
    "    u_mass = []\n",
    "    flags = []\n",
    "    for n, topic in enumerate(topics):\n",
    "        cm = CoherenceModel(topics=[topic], corpus=corpus, dictionary=dictionary, coherence='u_mass')\n",
    "        u_mass.append(cm.get_coherence())\n",
    "        return np.mean(u_mass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "files = [x for x in os.walk(\"../results/ABAE\")]\n",
    "\n",
    "scores = []\n",
    "for t in files[0][2]:\n",
    "    if t[:6] == \"abae-k\":\n",
    "        with open(os.path.join(\"../results/ABAE\",t)) as f:\n",
    "            aspects = json.load(f)\n",
    "\n",
    "        topics = [[ word for word, score in words.items() ] for aspect, words in aspects.items() ]\n",
    "\n",
    "        s = calc_coherance(topics)\n",
    "        scores.append((t, s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abae-k-13-orth-0.5.json'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "for score in scores:\n",
    "    x = score[0][:-5].split(\"-\")\n",
    "    result.append({\n",
    "        \"k\":x[2],\n",
    "        \"ortho\":x[4],\n",
    "        \"score\":score[1]\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(result).to_csv(\"../data/abae_umass.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>k</th>\n",
       "      <th>ortho</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>-15.972001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>1.3</td>\n",
       "      <td>-14.190212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-14.037156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-16.920428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14</td>\n",
       "      <td>1.3</td>\n",
       "      <td>-11.013497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>11</td>\n",
       "      <td>1.3</td>\n",
       "      <td>-11.087480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>14</td>\n",
       "      <td>0.8</td>\n",
       "      <td>-14.118206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-13.095088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>-13.418954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>13</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-13.169399</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>63 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     k ortho      score\n",
       "0    9     1 -15.972001\n",
       "1   10   1.3 -14.190212\n",
       "2    6   0.1 -14.037156\n",
       "3    9   0.5 -16.920428\n",
       "4   14   1.3 -11.013497\n",
       "..  ..   ...        ...\n",
       "58  11   1.3 -11.087480\n",
       "59  14   0.8 -14.118206\n",
       "60   5   0.3 -13.095088\n",
       "61  10     1 -13.418954\n",
       "62  13   0.5 -13.169399\n",
       "\n",
       "[63 rows x 3 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "LDA Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "k = 5\n",
    "ortho = 0.8\n",
    "with open(\"../results/ABAE/abae-k-{0}-orth-{1}.json\".format(str(k), str(ortho)), \"r\") as f:\n",
    "    apsects = json.load(f)\n",
    "    \n",
    "topics = [[ word for word, score in words.items() ] for aspect, words in apsects.items() ]\n",
    "\n",
    "topics_idx = [[ vocab[word] for word, score in words.items() ] for aspect, words in apsects.items() ] \n",
    "\n",
    "len(topics_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "vocab = Counter()\n",
    "\n",
    "for topic in topics:\n",
    "    for word in topic:\n",
    "        vocab[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('awesome', 2),\n",
       " ('great', 2),\n",
       " ('wonderful', 2),\n",
       " ('good', 2),\n",
       " ('management', 2),\n",
       " ('funny', 1),\n",
       " ('friendly', 1),\n",
       " ('really', 1),\n",
       " ('kind', 1),\n",
       " ('coworker', 1)]"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def termFreq(doc):\n",
    "    vocab = Counter()\n",
    "    for word in doc:\n",
    "        vocab[word] += 1\n",
    "        \n",
    "    return dict(vocab)\n",
    "\n",
    "reviews_tf = list(map(termFreq, train_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:11<00:00,  2.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta Shape (148425, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "document_sums = np.zeros((len(topics_idx),k))\n",
    "\n",
    "def countWords(topic, doc):\n",
    "    x = 0\n",
    "    for w in topic:\n",
    "        try:\n",
    "            x += doc[w]\n",
    "        except:\n",
    "            pass\n",
    "    return x\n",
    "\n",
    "for row, topic in tqdm(enumerate(topics_idx), total=len(topics_idx)):\n",
    "    for col, doc in enumerate(reviews_tf):\n",
    "        ct = countWords(topic, doc)\n",
    "        document_sums[row,col] = ct\n",
    "\n",
    "alpha = 1/k\n",
    "theta = []\n",
    "for i in range(k):\n",
    "    if document_sums[i,:].sum() == 0: break\n",
    "    theta.append(( document_sums[i,:] + alpha ) / document_sums[i,:].sum())\n",
    "    \n",
    "theta = np.array(theta).T\n",
    "\n",
    "print(\"Theta Shape\", theta.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 22791)"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_sums = np.zeros((len(vocab), k))\n",
    "\n",
    "def countWords(topic, doc):\n",
    "    x = 0\n",
    "    for w in topic:\n",
    "        try:\n",
    "            x += doc[w]\n",
    "        except:\n",
    "            pass\n",
    "    return x\n",
    "\n",
    "for row, topic in tqdm(enumerate(topics_idx), total=len(topics_idx)):\n",
    "    for col, doc in enumerate(reviews_tf):\n",
    "        ct = countWords(topic, doc)\n",
    "        document_sums[row,col] = ct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/derekgreene/topic-model-tutorial/blob/master/3%20-%20Parameter%20Selection%20for%20NMF.ipynb\n",
    "import gensim\n",
    "from itertools import combinations\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "w2v_model = gensim.models.Word2Vec.load(emb_filename)\n",
    "\n",
    "def getVector(w2v_model, word):\n",
    "    try:\n",
    "        v = w2v_model.wv[word]\n",
    "    except:\n",
    "        v = np.repeat(.00000001, w2v_model.vector_size)\n",
    "    return v\n",
    "\n",
    "def calculate_coherence( w2v_model, term_rankings ):\n",
    "    overall_coherence = 0.0\n",
    "    for topic_index in range(len(term_rankings)):\n",
    "        # check each pair of terms\n",
    "        pair_scores = []\n",
    "        for pair in combinations( term_rankings[topic_index], 2 ):\n",
    "            pair_scores.append( 1- cosine(getVector(w2v_model, pair[0]), getVector(w2v_model,pair[1])) )\n",
    "        # get the mean for all pairs in this topic\n",
    "        topic_score = sum(pair_scores) / len(pair_scores)\n",
    "        overall_coherence += topic_score\n",
    "    # get the mean score across all topics\n",
    "    return overall_coherence / len(term_rankings)\n",
    "\n",
    "calculate_coherence(w2v_model, topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk import WordNetLemmatizer\n",
    "\n",
    "lmtzr = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'night'"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lmtzr.lemmatize(\"nights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
