{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "final_review_topics = pd.read_csv(\"../data/final_review_topics.csv\")\n",
    "final_review_sentence_topics = pd.read_csv(\"../data/final_review_sentence_topics.csv\")\n",
    "\n",
    "review_annotate = pd.read_csv(\"../data/review_annotate.csv\")\n",
    "\n",
    "sentence_annotate = pd.read_csv(\"../data/sentence_annotate.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>lda_topic_label</th>\n",
       "      <th>label_name_primary</th>\n",
       "      <th>label_name_secondary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1897634</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>119324</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1803538</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1713596</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1873155</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     index  lda_topic_label  label_name_primary  label_name_secondary\n",
       "0  1897634                0                   0                     1\n",
       "1   119324                4                   1                     0\n",
       "2  1803538                3                   5                     1\n",
       "3  1713596                4                   7                     6\n",
       "4  1873155                5                   5                     0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "5    64\n",
       "4    58\n",
       "0    33\n",
       "6    31\n",
       "3    12\n",
       "7     9\n",
       "1     6\n",
       "Name: lda_topic_label, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = review_annotate.lda_topic_label == 2\n",
    "review_annotate.loc[idx, \"lda_topic_label\"] = 7\n",
    "\n",
    "display(review_annotate.head())\n",
    "\n",
    "review_annotate.lda_topic_label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.29577464788732394\n",
      "[[11  2  5  7  5  3  0]\n",
      " [ 1  2  2  0  0  0  1]\n",
      " [ 0  0  6  4  1  1  0]\n",
      " [14  7  7  9 12  4  5]\n",
      " [12  9 11  4 23  3  2]\n",
      " [ 6  1  2  2  8  9  3]\n",
      " [ 1  0  1  2  1  1  3]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.24      0.33      0.28        33\n",
      "           1       0.10      0.33      0.15         6\n",
      "           3       0.18      0.50      0.26        12\n",
      "           4       0.32      0.16      0.21        58\n",
      "           5       0.46      0.36      0.40        64\n",
      "           6       0.43      0.29      0.35        31\n",
      "           7       0.21      0.33      0.26         9\n",
      "\n",
      "    accuracy                           0.30       213\n",
      "   macro avg       0.28      0.33      0.27       213\n",
      "weighted avg       0.35      0.30      0.30       213\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y = review_annotate.lda_topic_label.values\n",
    "y_hat = review_annotate.label_name_primary.values\n",
    "\n",
    "print( accuracy_score(y, y_hat) )\n",
    "\n",
    "print( confusion_matrix(y, y_hat) )\n",
    "\n",
    "print( classification_report(y, y_hat) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4835680751173709"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat = []\n",
    "for i, row in review_annotate.iterrows():\n",
    "    if row.lda_topic_label == row.label_name_primary or row.lda_topic_label == row.label_name_secondary:\n",
    "        y_hat.append(1)\n",
    "    else:\n",
    "        y_hat.append(0)\n",
    "\n",
    "sum(y_hat) / len(y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.39344262295081966\n",
      "[[ 0  0  0  0  0  0]\n",
      " [ 3  4  4  2 10  0]\n",
      " [ 1  0 13  1 13  6]\n",
      " [ 1  0  6 10  9  1]\n",
      " [ 3  1  3  0 14  0]\n",
      " [ 1  1  6  1  1  7]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.00      0.00      0.00         0\n",
      "           0       0.67      0.17      0.28        23\n",
      "           1       0.41      0.38      0.39        34\n",
      "           2       0.71      0.37      0.49        27\n",
      "           3       0.30      0.67      0.41        21\n",
      "           4       0.50      0.41      0.45        17\n",
      "\n",
      "    accuracy                           0.39       122\n",
      "   macro avg       0.43      0.33      0.34       122\n",
      "weighted avg       0.52      0.39      0.40       122\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christopherballenger/py/miniconda3/envs/capstone/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "y = sentence_annotate.abae_topic_label.values\n",
    "y_hat = sentence_annotate.label_name_primary.values\n",
    "\n",
    "print( accuracy_score(y, y_hat) )\n",
    "\n",
    "print( confusion_matrix(y, y_hat) )\n",
    "\n",
    "print( classification_report(y, y_hat) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5245901639344263"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat = []\n",
    "for i, row in sentence_annotate.iterrows():\n",
    "    if row.abae_topic_label == row.label_name_primary or row.abae_topic_label == row.label_name_secondary:\n",
    "        y_hat.append(1)\n",
    "    else:\n",
    "        y_hat.append(0)\n",
    "\n",
    "sum(y_hat) / len(y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.0 64-bit ('capstone': conda)",
   "language": "python",
   "name": "python37064bitcapstoneconda433aed4549574e2f807458347f00c8aa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
